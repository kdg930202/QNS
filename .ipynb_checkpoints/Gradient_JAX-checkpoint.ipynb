{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "856beb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jacfwd, jacrev, grad\n",
    "import numpy as np\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a8a118",
   "metadata": {},
   "source": [
    "# The use of JVP (in case of forward mode) and VJP (in case of backward mode) is more memory efficient than computing the Jacobian directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://jax.readthedocs.io/en/latest/notebooks/autodiff_cookbook.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecc24ce",
   "metadata": {},
   "source": [
    "grad(f) = $\\nabla f$\n",
    "\n",
    "grad(f)(x) = $\\nabla f(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b372e5",
   "metadata": {},
   "source": [
    "## `jax.vjp`\n",
    "\n",
    "**A bit about math**\n",
    "\n",
    "Mathematically, suppose we have a function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, the Jacobian matrix of $f$ at a particular point $x$, denoted $J(x) \\in \\mathbb{R}^{m \\times n}$, is a matrix:\n",
    "$$J(x) = \n",
    "\\left(\\begin{matrix} \n",
    "\\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{matrix} \\right)$$\n",
    "\n",
    "You can think of it as a linear map $J(x): \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ which maps $v$ to $J(x)v$.\n",
    "\n",
    "What vector-Jacobian product does is to compute $vJ(x)$ or $J(x)^\\top v$. `jax.vjp` is the api to compute the vector-Jacobian product in JAX with two arguments:\n",
    "- first argument: a callable function $f$\n",
    "- second argument: primal value at which point the Jacobian is evaluated (Should be either a tuple or a list of arguments)\n",
    "\n",
    "It returns both $f(x)$ and a linear map $J(x)^\\top: \\mathbb{R}^m \\rightarrow \\mathbb{R}^n$ which map $v$ to $J^\\top v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f1771a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "f = lambda x: x**3 + 2*x**2 - 3*x + 1\n",
    "df = lambda x: 3*x**2 + 4*x - 3\n",
    "\n",
    "\n",
    "dfdx = jax.grad(f)\n",
    "d2fdx = jax.grad(dfdx)\n",
    "d3fdx = jax.grad(d2fdx)\n",
    "d4fdx = jax.grad(d3fdx)\n",
    "\n",
    "dsig = jax.grad(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acf7f6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "print(dfdx(1.))\n",
    "print(df(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e8d1f7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16. 12.]\n",
      " [-4. 18.]]\n"
     ]
    }
   ],
   "source": [
    "def ff(x):\n",
    "    x1, x2 = x[0], x[1]\n",
    "    return jnp.array([x1**4 + 3 * x2**2 *x1, 5*x2**2 - 2*x1*x2+1])\n",
    "\n",
    "\n",
    "J = jacrev(ff)\n",
    "\n",
    "x = jnp.array([1.0, 2.0])\n",
    "print(J(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cc5282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
